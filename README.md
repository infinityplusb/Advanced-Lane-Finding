## Advanced Lane Finding
[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)
---

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.



[//]: # (Image References)

[cal_img_undist]: ./output_images/calibration_image_undistortion.png "Undistorted Calibration Image"

[image1]: ./output_images/straight_lines2_undistortion.png "Undistorted of Straight Line Image"
[image2]: ./output_images/output_test5_undistortion.png "Undistorted of Road Curving Image"

[warp]: ./output_images/warp_orig_screenshot_07.03.2019.png "Warped Bounding Box"
[thresh]: ./output_images/threshold_orig_screenshot_07.03.2019.png "Thresholds"

[image4]: ./examples/warped_straight_lines.jpg "Warp Example"
[image5]: ./examples/color_fit_lines.jpg "Fit Visual"
[image6]: ./examples/example_output.jpg "Output"
[video1]: ./project_video.mp4 "Video"

## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points

### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---

### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  

You're reading it!
The entire project logic is contained in [lane_finder.py](link).
In this file, I have calculated the calibration matrix, then proceeded to create a sequence of functions for each needed component of the process.
At the end of the file, lines #359 - #416 contains a function that contains all the required function in a logical order.
Lines 425 - 437 calls the function for files
Lines 440 - 447 calls the function for videos

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

Using the images designed for the calibration, lines #47 - #81 read in and process the images.
This is done by converting to grayscale (line #51), and then applying the findChessboardCorners function to the image (line #56).
Assuming the chessboard corners are found, we can create object points and image points,
and use these to both calibrate the camera lens (line #78 to call calibrate camera on #29)

![Calibration Image][cal_img_undist]

### Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

The distortion-correction is applied to the image during the thresholding function as described in the new step.
Essentially an image is read in, and passed to the thresholding function. During this phase, the image is converted to gray scale (line #116), and the objpoints and imgpoints calculated in the previous section are used to undistort the image (line #117). This is then used for further processing. However, there is little impact between the results generated by undistorted versus distorted images, most likely due to the use of a bounding box to generate areas of interest.
![alt text][image1]
![alt text][image2]

#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

The thresholding function is broken down into sections.
##### 1. Converting the original image to HLS space (lines #99 - #107)
##### 2. Converting the original image to grayscale (lines #116 - #117)
##### 3. Calculating Absolute X and Y Sobel values (lines #133 - #145)
##### 4. Calculating the Magnitude Sobel values (lines #148 - #158)
##### 5. Calculating the Directional Sobel values (lines #162 - #167)
##### 6. Output the thresholded image (lines #162 - #167)
The rule of which thresholding to use was either use where abs values of X and Y overlapped (step 3), or where the Magnitude and Directional values overlapped. Presumably it will always take the absolute values overlapping over a potentially better direction/magnitude combo, due to the ordering. Some work could be done to ascertain which is better.

A specific timesink for me was that some images, and previous examples in the notes had scaled thresholds = 1, whereas some of my images were 0-1 scale and some were 0-255.
As such, some of my binarisations needed to be `=1` and some were `=255`. This is a quick fix approach. Going forward, some work will be done to ensure all images are similarly scaled.

![Threshold Image][thresh]

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

For the perspective transform, I took the bounding box where I presumed lines would mostly be (in front of the car for a distance almost to the horizon) and performed my perspective transform specifically on this bounding box.
This transform is captured in the calculate_M function (lines #216 - #220) and using that M, is called in the following functions corner_unwarp and corner_warp.
These are called to change to a birdseye view, and back to perspective view respectively.

![Warp of the Bounding Box][warp]

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

Then I did some other stuff and fit my lane lines with a 2nd order polynomial kinda like this:

![alt text][image5]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

I did this in lines # through # in my code in `my_other_file.py`

#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

I implemented this step in lines # through # in my code in `yet_another_file.py` in the function `map_lane()`.  Here is an example of my result on a test image:

![alt text][image6]

---

### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

Here's a [link to my video result](./project_video.mp4)

---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

Thresholding:
Presumably it will always take the absolute values overlapping over a potentially better direction/magnitude combo, due to the ordering. Some work could be done to ascertain which is better.
Scaling:
